https://lumagallacio.medium.com/tutorial-como-instalar-apache-airflow-com-docker-ab818d1a55e6

Invoke-WebRequest -Uri 'https://airflow.apache.org/docs/apache-airflow/2.8.0/docker-compose.yaml' -OutFile 'docker-compose.yaml' -Force

New-Item -ItemType Directory -Path .\dags, .\logs, .\plugins, .\config

$env:AIRFLOW_UID = [System.Environment]::UserInteractive

Para exportar a variável de ambiente:
export AIRFLOW_HOME=$(pwd)/[pasta do projeto]

docker compose up airflow-init
poetry run docker compose up airflow-init

docker-compose up -d
poetry run docker-compose up -d

docker ps
poetry run docker ps

Abra um navegador e digite a URL local: http://localhost:8080. Prrencha o nome de usuário como “airflow” e a senha como “airflow”.

docker compose restart airflow-webserver
poetry run docker compose restart airflow-webserver

poetry run docker-compose down
docker-compose down

Alguns dos operadores mais usados são:
 PythonOperator: executa uma função Python;
 BashOperator: executa um script bash;
 KubernetesPodOperator: executa uma imagem definida como imagem do Docker em um pod do Kubernetes;
 SnowflakeOperator: executa uma consulta em um banco de dados Snowflake;
 EmailOperator: envia um e-mail.

Para saber mais: base Operator

Essa é a classe base para todos os operadores. Os operadores criam objetos que se tornam parte do processo, o BaseOperator contém muitos métodos que são importantes para o comportamento de rastreamento do processo. Para herdar essa classe, espera-se que você substitua o construtor e também o método execute.
Os operadores derivados desta classe devem executar ou acionar determinadas tarefas de forma síncrona (aguardar a conclusão). As instâncias desses operadores (tarefas) visam operações específicas, executando scripts, funções ou transferências de dados específicos.
Recomendamos que sempre busque uma solução já pronta porque já existem muitos provedores disponíveis na biblioteca do Airflow, mas caso sua tarefa seja específica, você deve utilizar o BaseOperator para criar seu operador. Você pode se aprofundar nesse conceito pela documentação.

O Airflow tem um conjunto muito extenso de operadores disponíveis. 
No entanto, se não existir um operador para seu caso de uso, você também pode criar os seus próprios operadores.

Um dos recursos mais fundamentais do Apache Airflow é a capacidade de agendar trabalhos. Dessa forma, é importante conhecermos alguns termos do Airflow que são relacionados ao agendamento:

  Data de início (start date): data em que o DAG começa a ser agendado. Essa data é o início do intervalo de dados que você deseja processar;
  Intervalo de agendamento (schedule interval): define o intervalo de tempo em que o DAG é acionado. Por exemplo, "@daily" significa que o DAG deve ser acionado todos os dias à meia-noite;
  Intervalo de dados (data interval): propriedade de cada DAG Run que representa o intervalo de tempo em que cada tarefa deve operar. Por exemplo, para um DAG agendado de dia em dia, cada intervalo de dados começará no início (00:00) e terminará no final do dia (23:59). A execução do DAG acontece no final do intervalo de dados;
  Data lógica (logical date): é a mesma do início do intervalo de dados. Não representa quando o DAG será realmente executado. Antes do Airflow 2.2, era chamada de data de execução.

O Airflow utiliza o Jinja, uma estrutura de modelagem em Python, como seu mecanismo de modelagem. Vamos conhecer mais algumas variáveis que podemos utilizar em tempo de execução:

  data_interval_start: data do início do intervalo de dados;
  data_interval_end: data do fim do intervalo de dados;
  ds: data lógica de execução do DAG;
  ds_nodash: data lógica de execução do DAG sem nenhuma separação por traços.

Cron Expressions:
 minuto: 0-59;
 hora: 0-23;
 dia do mês: 1-31;
 mês: 1-12;
 dia da semana: 0-6 representando de domingo a sábado.

XCom:
O XCom é um recurso nativo do Airflow para compartilhar dados de tarefas. Esse recurso permite que as tarefas troquem metadados de tarefas ou pequenas quantidades de dados. Eles são definidos por chave, valor e data.
Os XComs podem ser enviados (xcom.push) ou recebidos (xcom.pull) por uma tarefa. Quando enviado por uma tarefa, ele é armazenado no banco de dados do Airflow e fica disponível para todas as outras tarefas.
Esse recurso deve ser utilizado apenas para passar pequenas quantidades de dados entre as tarefas. Ele não foi projetado para passar dados como DataFrames ou semelhantes. Para casos como esse, podemos utilizar o armazenamento de dados intermediário, que é mais apropriado para grandes blocos de dados. 
